{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "#to provide configuration options such as Pipeline Runner (DataFlow), import the packages\n",
    "from apache_beam.options.pipeline_options import PipelineOptions,StandardOptions\n",
    "\n",
    "#python library to support command line arguments. Required to provide the input file name in the \n",
    "#pipeline run command\n",
    "import argparse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--input'], dest='input', nargs=None, const=None, default=None, type=None, choices=None, help='Input file to process', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a object of argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "#Pass the file like --input file.xlsx ---- This file can be accessed by input -- True means mandatory arguments\n",
    "parser.add_argument('--input',dest='input',required=True,help = 'Input file to process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline args hold information for environment variables like runner type, job name, temporary file location etc.\n",
    "#path_args will hold the input file location\n",
    "path_args,pipeline_args = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To use input file location in the code\n",
    "inputs_pattern = path_args.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#options will hold all the environment configurations. Will be passed to the actual pipeline object\n",
    "options = PipelineOptions(pipeline_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p is like Spark Context. All the transformations will be applied to p.\n",
    "p = beam.pipeline(options = options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation in Beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_data is the p collection here. p collection is just like RDDs and Dataframes in Spark\n",
    "#p collection can hold any Batch Or Streaming Data\n",
    "cleaned_data = (\n",
    "    p\n",
    "    |\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
